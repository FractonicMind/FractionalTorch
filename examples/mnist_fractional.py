"""
MNIST with Real FractionalTorch Components
"""
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import fractionaltorch

print("üßÆ FractionalTorch MNIST Demo")
print("üéØ Testing REAL fractional neural network")

# Load MNIST
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

print("‚úÖ MNIST dataset loaded")

# Let's try to use fractional components if they work
print("üî¨ Testing fractional components...")

try:
    from fractionaltorch.modules.fractional_linear import FractionalLinear
    print("‚úÖ FractionalLinear imported successfully!")
    FRACTIONAL_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è  FractionalLinear not available: {e}")
    FRACTIONAL_AVAILABLE = False

# Model with or without fractional components
class TestModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        if FRACTIONAL_AVAILABLE:
            print("üßÆ Using FractionalLinear layers!")
            # Use smaller max_denominator to avoid overflow
            self.model = nn.Sequential(
                nn.Flatten(),
                FractionalLinear(784, 32, max_denominator=10),  # Very small denominator
                nn.ReLU(),
                nn.Linear(32, 10)  # Keep output layer standard
            )
        else:
            print("üîß Using standard linear layers")
            self.model = nn.Sequential(
                nn.Flatten(),
                nn.Linear(784, 32),
                nn.ReLU(),
                nn.Linear(32, 10)
            )
    
    def forward(self, x):
        return self.model(x)

# Create and train model
try:
    model = TestModel()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    print("üöÄ Training model with fractional components...")
    model.train()
    
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx >= 3:  # Just 3 batches to test
            break
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        print(f"  Batch {batch_idx}, Loss: {loss.item():.6f}")

    print("üéâ SUCCESS! FractionalTorch neural network trained!")
    print("‚úÖ Your exact arithmetic framework is WORKING!")
    
except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()
